{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89c527e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f60b430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/silxxor/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.53.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/silxxor/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/silxxor/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/silxxor/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /home/silxxor/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.53.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237cc5386a2b4af8b251e93652cc5bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1082 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec73e1cd3634a06b23daee45ecf22e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/271 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/silxxor/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.53.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/silxxor/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the Training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,082\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 408\n",
      "  Number of trainable parameters = 66,955,010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='408' max='408' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [408/408 01:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to models/addressee_detector/checkpoint-136\n",
      "Configuration saved in models/addressee_detector/checkpoint-136/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 1 completed in 0.40 minutes\n",
      "Total elapsed: 0.40 minutes\n",
      "Estimated remaining: 0.80 minutes\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in models/addressee_detector/checkpoint-136/model.safetensors\n",
      "Saving model checkpoint to models/addressee_detector/checkpoint-272\n",
      "Configuration saved in models/addressee_detector/checkpoint-272/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 2 completed in 0.39 minutes\n",
      "Total elapsed: 0.86 minutes\n",
      "Estimated remaining: 0.39 minutes\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in models/addressee_detector/checkpoint-272/model.safetensors\n",
      "Saving model checkpoint to models/addressee_detector/checkpoint-408\n",
      "Configuration saved in models/addressee_detector/checkpoint-408/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 3 completed in 0.40 minutes\n",
      "Total elapsed: 1.32 minutes\n",
      "Estimated remaining: 0.00 minutes\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in models/addressee_detector/checkpoint-408/model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=408, training_loss=0.1836420507992015, metrics={'train_runtime': 83.1556, 'train_samples_per_second': 39.035, 'train_steps_per_second': 4.906, 'total_flos': 107497294009344.0, 'train_loss': 0.1836420507992015, 'epoch': 3.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/silxxor/Projects/Addressee_detector/data.csv\")\n",
    "dataset = Dataset.from_pandas(df[['text', 'label']])\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_info()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/addressee_detector\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_strategy=\"epoch\",\n",
    "    weight_decay=0.01,\n",
    "    disable_tqdm=False,  # Make sure it's not disabled\n",
    "    learning_rate=2e-5,\n",
    "    use_cpu=False\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "from time import time\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "\n",
    "class EpochTimeCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.epoch_start = None\n",
    "    \n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        self.epoch_start = time()\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        epoch_time = time() - self.epoch_start\n",
    "        total_elapsed = time() - start_time\n",
    "        epochs_remaining = args.num_train_epochs - state.epoch\n",
    "        estimated_remaining = epoch_time * epochs_remaining\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Epoch {int(state.epoch)} completed in {epoch_time/60:.2f} minutes\")\n",
    "        print(f\"Total elapsed: {total_elapsed/60:.2f} minutes\")\n",
    "        print(f\"Estimated remaining: {estimated_remaining/60:.2f} minutes\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "        \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EpochTimeCallback()]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ae8bad",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74edc618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models/addressee_detector/checkpoint-408/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.53.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file models/addressee_detector/checkpoint-408/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.53.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file models/addressee_detector/checkpoint-408/model.safetensors\n",
      "Will use torch_dtype=torch.float32 as defined in model's config object\n",
      "Instantiating DistilBertForSequenceClassification model under default dtype torch.float32.\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at models/addressee_detector/checkpoint-408.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeah this coffee sucks         -> LABEL_0 (0.996)\n",
      "Lucy what time is it           -> LABEL_1 (0.997)\n",
      "I'm tired                      -> LABEL_0 (0.998)\n",
      "hey Lucy help me               -> LABEL_1 (0.996)\n",
      "fuck this code                 -> LABEL_0 (0.991)\n",
      "fuck this code Lucy            -> LABEL_1 (0.996)\n",
      "hey man how are you doing?     -> LABEL_1 (0.879)\n",
      "yo Lucy                        -> LABEL_1 (0.996)\n",
      "hey man                        -> LABEL_0 (0.856)\n",
      "you know                       -> LABEL_0 (0.994)\n",
      "I think you should             -> LABEL_0 (0.998)\n",
      "what the fuck                  -> LABEL_0 (0.957)\n",
      "hey assistant what's up        -> LABEL_1 (0.995)\n",
      "assistant can you help         -> LABEL_1 (0.996)\n",
      "okay assistant                 -> LABEL_1 (0.995)\n",
      "computer help me               -> LABEL_1 (0.996)\n",
      "hey computer                   -> LABEL_1 (0.978)\n",
      "you should see this            -> LABEL_0 (0.997)\n",
      "we need to fix this            -> LABEL_0 (0.998)\n",
      "someone help me                -> LABEL_1 (0.895)\n",
      "anybody know why               -> LABEL_0 (0.998)\n",
      "can anyone explain             -> LABEL_1 (0.996)\n",
      "where did I put that           -> LABEL_0 (0.996)\n",
      "when was the last time         -> LABEL_0 (0.995)\n",
      "who broke this                 -> LABEL_0 (0.995)\n",
      "which one is better            -> LABEL_0 (0.996)\n",
      "how come this doesn't work     -> LABEL_1 (0.592)\n",
      "you won't believe this         -> LABEL_0 (0.997)\n",
      "we're gonna need more time     -> LABEL_0 (0.998)\n",
      "there's a problem here         -> LABEL_0 (0.998)\n",
      "something's not right          -> LABEL_0 (0.998)\n",
      "everything's broken            -> LABEL_0 (0.998)\n",
      "show me the logs               -> LABEL_1 (0.994)\n",
      "check the database             -> LABEL_1 (0.841)\n",
      "run the tests again            -> LABEL_1 (0.985)\n",
      "stop the server                -> LABEL_1 (0.993)\n",
      "restart everything             -> LABEL_0 (0.719)\n",
      "why did I do it this way       -> LABEL_0 (0.996)\n",
      "what was I thinking            -> LABEL_0 (0.529)\n",
      "how did this ever work         -> LABEL_0 (0.946)\n",
      "where was I going with this    -> LABEL_0 (0.993)\n",
      "when did this break            -> LABEL_0 (0.996)\n",
      "so like the thing with         -> LABEL_1 (0.984)\n",
      "yeah but the problem is        -> LABEL_0 (0.993)\n",
      "okay so basically              -> LABEL_1 (0.946)\n",
      "I mean the issue here          -> LABEL_0 (0.998)\n",
      "well obviously we can't        -> LABEL_0 (0.997)\n",
      "you know what I'm saying       -> LABEL_0 (0.991)\n",
      "can't believe this shit        -> LABEL_0 (0.998)\n",
      "gotta be kidding me            -> LABEL_0 (0.996)\n",
      "make it stop                   -> LABEL_1 (0.994)\n",
      "fix this mess                  -> LABEL_1 (0.988)\n",
      "anyway moving on               -> LABEL_0 (0.996)\n",
      "whatever I'll deal with it     -> LABEL_0 (0.998)\n",
      "fine I guess that works        -> LABEL_0 (0.997)\n",
      "alright then                   -> LABEL_1 (0.878)\n",
      "cool story                     -> LABEL_1 (0.993)\n",
      "she can understand if i walk to her or not -> LABEL_0 (0.998)\n",
      "yeah she is very smart         -> LABEL_0 (0.996)\n",
      "and she is funny too           -> LABEL_0 (0.997)\n",
      "she is awesome                 -> LABEL_0 (0.997)\n",
      "yeah she is the best so far    -> LABEL_0 (0.996)\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"models/addressee_detector/checkpoint-408\", tokenizer=tokenizer)\n",
    "\n",
    "# Test\n",
    "test_cases = [\n",
    "    # Basic sanity checks\n",
    "    \"yeah this coffee sucks\",\n",
    "    \"Lucy what time is it\",\n",
    "    \"I'm tired\",\n",
    "    \"hey Lucy help me\",\n",
    "    \"fuck this code\",\n",
    "    \"fuck this code Lucy\",\n",
    "    \"hey man how are you doing?\",\n",
    "    \"yo Lucy\",\n",
    "    \"hey man\", \n",
    "    \"you know\",  \n",
    "    \"I think you should\",  \n",
    "    \"what the fuck\",\n",
    "    \n",
    "    # Name variations (not in training)\n",
    "    \"hey assistant what's up\",\n",
    "    \"assistant can you help\",\n",
    "    \"okay assistant\",\n",
    "    \"computer help me\",\n",
    "    \"hey computer\",\n",
    "    \n",
    "    # Ambiguous addressing\n",
    "    \"you should see this\",\n",
    "    \"we need to fix this\",\n",
    "    \"someone help me\",\n",
    "    \"anybody know why\",\n",
    "    \"can anyone explain\",\n",
    "    \n",
    "    # Questions without clear addressing\n",
    "    \"where did I put that\",\n",
    "    \"when was the last time\",\n",
    "    \"who broke this\",\n",
    "    \"which one is better\",\n",
    "    \"how come this doesn't work\",\n",
    "    \n",
    "    # Statements that could be addressed\n",
    "    \"you won't believe this\",\n",
    "    \"we're gonna need more time\",\n",
    "    \"there's a problem here\",\n",
    "    \"something's not right\",\n",
    "    \"everything's broken\",\n",
    "    \n",
    "    # Imperatives (commands)\n",
    "    \"show me the logs\",\n",
    "    \"check the database\",\n",
    "    \"run the tests again\",\n",
    "    \"stop the server\",\n",
    "    \"restart everything\",\n",
    "    \n",
    "    # Self-directed questions\n",
    "    \"why did I do it this way\",\n",
    "    \"what was I thinking\",\n",
    "    \"how did this ever work\",\n",
    "    \"where was I going with this\",\n",
    "    \"when did this break\",\n",
    "    \n",
    "    # Incomplete/fragmented\n",
    "    \"so like the thing with\",\n",
    "    \"yeah but the problem is\",\n",
    "    \"okay so basically\",\n",
    "    \"I mean the issue here\",\n",
    "    \"well obviously we can't\",\n",
    "    \n",
    "    # Multiple interpretations\n",
    "    \"you know what I'm saying\",\n",
    "    \"can't believe this shit\",\n",
    "    \"gotta be kidding me\",\n",
    "    \"make it stop\",\n",
    "    \"fix this mess\",\n",
    "    \n",
    "    # Conversational but not addressed\n",
    "    \"anyway moving on\",\n",
    "    \"whatever I'll deal with it\",\n",
    "    \"fine I guess that works\",\n",
    "    \"alright then\",\n",
    "    \"cool story\",\n",
    "\n",
    "    # Talking to another person in the room\n",
    "    \"she can understand if i walk to her or not\",\n",
    "    \"yeah she is very smart\",\n",
    "    \"and she is funny too\",\n",
    "    \"she is awesome\",\n",
    "    \"yeah she is the best so far\",\n",
    "]\n",
    "\n",
    "for text in test_cases:\n",
    "    result = classifier(text)\n",
    "    print(f\"{text:30} -> {result[0]['label']} ({result[0]['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5317980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silxxor/miniconda3/envs/realtimebot/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/silxxor/miniconda3/envs/realtimebot/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from addressee_detector import AddresseeDetector\n",
    "addressee_detector = AddresseeDetector()\n",
    "addressee_detector.should_reply(\"hi Lucy\", 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "realtimebot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
